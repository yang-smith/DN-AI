


import asyncio
from datetime import datetime
import os
import re
from openai import OpenAI

from knowledge_graph import KnowledgeGraph


text = """
å¤§å‘¨_ä½™æ‘: @é˜¿RUIâ€…å¸Œæœ›ä¸­å›½æ•°å­—æ¸¸æ°‘ç¤¾åŒºä¹Ÿèƒ½æˆä¸ºä¸­è€å¹´çš„ä¹Œæ‰˜é‚¦ ï¼Œå¸Œæœ›ä¸­å¹´äººèƒ½å¤Ÿåœ¨é’å¹´äººè¿·èŒ«æ—¶åˆ†äº«è‡ªå·±çš„ç»å†å’Œæ€è€ƒï¼Œå†è¿‡å‡ å¹´ ï¼Œæœ‰è¡ŒåŠ¨èƒ½åŠ›çš„è€å¹´äººä¹Ÿèƒ½æ¥ç¤¾åŒºæ—…å±…åšä¹‰å·¥
å¤§å‘¨_ä½™æ‘: è¿™æ˜¯ä¸æ˜¯åœ¨æ¢ç´¢å…±äº§ä¸»ä¹‰å¤§å›¢ç»“çš„å®ç°æ–¹å¼
é˜¿RUI: æŒ‡æ—¥å¯å¾…ï¼ ä¹Œæ‰˜é‚¦æ²¡æœ‰å‰ç¼€ï¼Œå°±æ˜¯ä¹Œæ‰˜é‚¦
å¼•ç”¨:å¤§å‘¨:@é˜¿RUIâ€…å¸Œæœ›ä¸­å›½æ•°å­—æ¸¸æ°‘ç¤¾åŒºä¹Ÿèƒ½æˆä¸ºä¸­è€å¹´çš„ä¹Œæ‰˜é‚¦ï¼Œå¸Œæœ›ä¸­å¹´äººèƒ½å¤Ÿåœ¨é’å¹´äººè¿·èŒ«æ—¶åˆ†äº«è‡ªå·±çš„ç»å†å’Œæ€è€ƒï¼Œå†è¿‡å‡ å¹´ï¼Œ æœ‰è¡ŒåŠ¨èƒ½åŠ›çš„è€å¹´äººä¹Ÿèƒ½æ¥ç¤¾åŒºæ—…å±…åšä¹‰å·¥
å¤§å‘¨_ä½™æ‘: å—¯å—¯ï¼Œä¸è¿‡è§‚å¯Ÿä¸‹æ¥ï¼Œå¹´è½»äººæ¥ä¹Œæ‰˜é‚¦é‡Œå¯»æ‰¾çˆ±ä¹Ÿæ˜¯ä¸€å¤§è¶‹åŠ¿[å‘²ç‰™]
å¼•ç”¨:æœªçŸ¥
Lili: æˆ‘ä»£è¡¨ä¸­è€å¹´åœ¨è¿™é‡Œæ‰æ ¹â€¦
å¼•ç”¨:å¤§å‘¨:@é˜¿RUIâ€…å¸Œæœ›ä¸­å›½æ•°å­—æ¸¸æ°‘ç¤¾åŒºä¹Ÿèƒ½æˆä¸ºä¸­è€å¹´çš„ä¹Œæ‰˜é‚¦ï¼Œå¸Œæœ›ä¸­å¹´äººèƒ½å¤Ÿåœ¨é’å¹´äººè¿·èŒ«æ—¶åˆ†äº«è‡ªå·±çš„ç»å†å’Œæ€è€ƒï¼Œå†è¿‡å‡ å¹´ï¼Œ æœ‰è¡ŒåŠ¨èƒ½åŠ›çš„è€å¹´äººä¹Ÿèƒ½æ¥ç¤¾åŒºæ—…å±…åšä¹‰å·¥
ç»“å·´çš„Frank: å¦‚æœè¯´è®©è€äººæ¥å…»è€è¿˜æœ‰å·å¬åŠ›ï¼Œå‘¼åè®©è€äººåœ¨åšä¹‰å·¥ï¼Œè¿™ã€‚ã€‚ã€‚æˆ‘éƒ½ä»¥ä¸ºçœ‹é”™äº†ã€‚ã€‚ã€‚
å¼•ç”¨:å¤§å‘¨:@é˜¿RUIâ€…å¸Œæœ›ä¸­å›½æ•°å­—æ¸¸æ°‘ç¤¾åŒºä¹Ÿèƒ½æˆä¸ºä¸­è€å¹´çš„ä¹Œæ‰˜é‚¦ï¼Œå¸Œæœ›ä¸­å¹´äººèƒ½å¤Ÿåœ¨é’å¹´äººè¿·èŒ«æ—¶åˆ†äº«è‡ªå·±çš„ç»å†å’Œæ€è€ƒï¼Œå†è¿‡å‡ å¹´ï¼Œ æœ‰è¡ŒåŠ¨èƒ½åŠ›çš„è€å¹´äººä¹Ÿèƒ½æ¥ç¤¾åŒºæ—…å±…åšä¹‰å·¥
"""
text2 = """
K_C_: äº‘å—æ»‡å‘³å›­ç±³çº¿/çƒ§çƒ¤ä»Šæ—¥å¼€ä¸šäº†å•Šï¼ç­‰äº†å¾ˆä¹…çš„æœ‹å‹ä»¬å¯ä»¥å»å•¦ï¼
Lili: ä¹‹å‰æ²¡å¼€å—ï¼Ÿ
å¼•ç”¨:K.C.ï¼ˆç¤¾æç‰ˆï¼‰:äº‘å—æ»‡å‘³å›­ç±³çº¿/çƒ§çƒ¤ä»Šæ—¥å¼€ä¸šäº†å•Šï¼ç­‰äº†å¾ˆä¹…çš„æœ‹å‹ä»¬å¯ä»¥å»å•¦ï¼
Lili: æ™šä¸Šå»ï¼
K_C_: è€æ¿å‰å¤©åˆšå›å¤©è’åªï¼
å¼•ç”¨:æœªçŸ¥
"""

text3 = """
å¦‚é£_ä½™æ‘: è¯·æ•™ä¸€ä¸‹å¤§å®¶ å“ªé‡Œçš„å…¬å…±ç©ºé—´å¯ä»¥å¹å¤´å‘å‘€
Casey: å¹é£æœºæ‹¿åˆ°å¤–é¢éšä¾¿å¹å‘—
ç¦å°”æ‘©ç‘_ä½™æ‘: å»a3çš„äººæ€§äº¤æµå¸‚
æœ¨ä¸€å‡¡: å¯ä»¥a3å¨±ä¹å®¤
è¶æ—©âœ¨_ä½™æ‘: a2å«ç”Ÿé—´
å¼•ç”¨:å¦‚é£:è¯·æ•™ä¸€ä¸‹å¤§å®¶ å“ªé‡Œçš„å…¬å…±ç©ºé—´å¯ä»¥å¹å¤´å‘å‘€
å¦‚é£_ä½™æ‘: è°¢è°¢å¤§å®¶ A2æœ‰å¥½å¤šäººåœ¨åŠå…¬ A3å¨±ä¹å®¤ç¡®å®ä¸é”™ï¼      
å¦‚é£_ä½™æ‘: å…¬å…±æµ´å®¤èƒ½æ¥ä¸ªç”µæº+é•œå­å°±æ›´å¥½å•¦ï¼
"""
def prompt_summary(text):
    prompt = """ä½ çš„ä»»åŠ¡æ˜¯å°†ä¸‹é¢çš„èŠå¤©è®°å½•è½¬ä¸ºæ–¹ä¾¿è®°å¿†çš„ä¸€æ®µè¯ã€‚
    è¯·å°½é‡ä¿ç•™å¤§éƒ¨åˆ†ä¿¡æ¯ã€‚ä¿ç•™ç›¸å…³çš„äººå’Œæ—¶é—´ã€‚
    -Real Data-
    records:
    ```
    {}
    ```
    """
    return prompt.format(text)

import prompt_entities

def prompt_extract(text):
    prompt = """Your task is to extract named entities from the given paragraph. 
    Respond with a JSON list of entities.(in language of the paragraphs)
    entitiesåŒ…æ‹¬äººåã€æ´»åŠ¨åç§°ã€åœ°ç‚¹å’Œå…·ä½“äº§å“ã€‚è¯·ä»æ„å»ºçŸ¥è¯†å›¾è°±è§’åº¦è€ƒé‡ã€‚
    One-Shot Demonstration: 
    Paragraph:
    ```
    åœ¨2024å¹´4æœˆ9æ—¥çš„èŠå¤©è®°å½•ä¸­ï¼Œé˜¿é“¶Kuroå‘èµ·äº†ä¸€ä¸ªæ¥é¾™æ´»åŠ¨ï¼Œé‚€è¯·
    å¤§å®¶åœ¨19:30åˆ°21:30çš„B1è®¨è®ºå®¤ä¸€èµ·è®¨è®ºåŠ¨æ¼«ã€Šæ±Ÿæˆ·ç›—è´¼å›¢äº”å¶ã€‹ï¼Œ å¼ºè°ƒè¿™éƒ¨ä½œå“çš„æ‚ é—²å™äº‹é£æ ¼ã€‚å‚ä¸è€…åŒ…æ‹¬é£æ¸…å’Œå¼ å¯èŒµã€‚

    éšåï¼ŒåˆYXå’ŒSTELLALALAæ¥é¾™åˆ†äº«äº†ä»–ä»¬è®¤ä¸ºæœ€å¥½åƒçš„æŠ¹èŒ¶å’Œå¯å¯ç‘ å£«å·ï¼Œä»·æ ¼ä¸º6.5å…ƒä¸€å—ï¼Œå‚ä¸è€…æœ‰å°åˆã€å¤§çŒ«ã€æ™¨å®¸ã€å°è™ã€é˜¿é“¶å’ŒJay02ã€‚

    æ™šä¸Š10ç‚¹å¤šï¼ŒğŸ“Ÿ77KUKU77_ä½™æ‘å‘èµ·äº†å¦ä¸€ä¸ªæ¥é¾™æ´»åŠ¨ï¼Œä¸»é¢˜æ˜¯â€œä»¥é˜²ä¸‡ä¸€ä½ é¥¿äº†â€ï¼Œæä¾›è„†çš®æ·€ç²‰è‚ å’Œè„†çš®å¹´ç³•ï¼Œå‚ä¸è€…åŒ…æ‹¬é£æ¸…æ°” æ­ªã€æ¿æ —ã€æ¨å…‰ã€ç´«è–‡å’ŒCaseyï¼Œå¤§å®¶å¯ä»¥è‡ªå¸¦æ¯å­äº«ç”¨è‡ªåŠ©å¯ä¹ã€‚
    ```
    {{
    "entities": [
    {{"name": "é˜¿é“¶Kuro", "type": "äººç‰©"}},
    {{"name": "B1è®¨è®ºå®¤", "type": "åœ°ç‚¹"}},
    {{"name": "æ±Ÿæˆ·ç›—è´¼å›¢äº”å¶", "type": "åŠ¨æ¼«"}},
    {{"name": "æŠ¹èŒ¶", "type": "é£Ÿç‰©", "price": "6.5å…ƒä¸€å—"}},
    {{"name": "å¯å¯ç‘å£«å·", "type": "é£Ÿç‰©", "price": "6.5å…ƒä¸€å—"}},
    {{"name": "è„†çš®æ·€ç²‰è‚ ", "type": "é£Ÿç‰©"}},
    {{"name": "è„†çš®å¹´ç³•", "type": "é£Ÿç‰©"}},
    ]
    }}

    -Real Data-
    Paragraph:
    ```
    {}
    ```
    """
    prompt = prompt_entities.UNTYPED_ENTITY_RELATIONSHIPS_GENERATION_PROMPT
    return prompt.format(text)


def ai_chat(message, model="gpt-3.5-turbo", response_format = 'NOT_GIVEN'):
    client = OpenAI(
        # This is the default and can be omitted
        api_key=os.environ.get("OPENAI_API_KEY"),
        base_url=os.environ.get("OPENAI_API_BASE"),
    )
    messages=[
        {
            "role": "system", 
            "content": "You are a helpful assitant."
        },
        {
            "role": "user",
            "content": message,
        }
    ]
    if response_format == 'json':
        chat_completion = client.chat.completions.create(
            messages=messages,
            response_format=  { "type": "json_object" },
            model=model,
            temperature=0.05
        )
    else:
        chat_completion = client.chat.completions.create(
            messages=messages,
            model=model,
            temperature=0.05
        )
    return chat_completion.choices[0].message.content

import graph_extractor
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import networkx as nx

font_path = 'C:\\Windows\\Fonts\\simhei.ttf'  # ç¡®ä¿è·¯å¾„æ­£ç¡®
prop = fm.FontProperties(fname=font_path)

def visualize_and_save_graph(graph: nx.Graph, file_path: str):
    
    pos = nx.spring_layout(graph)  # ä¸ºå›¾å½¢çš„èŠ‚ç‚¹è®¾ç½®å¸ƒå±€
    nx.draw(graph, pos, with_labels=False, node_color='skyblue', edge_color='#FF5733', node_size=40, font_size=5)
    
    # ä½¿ç”¨æ­£ç¡®çš„å‚æ•°è°ƒç”¨
    labels = nx.draw_networkx_labels(graph, pos, font_family=prop.get_name(), font_size=5)
    
    # ä¿å­˜å›¾å½¢åˆ°æ–‡ä»¶
    plt.savefig(file_path)
    plt.close()
    nx.write_graphml(graph, "./graphtest.graphml")



async def main():
    kg = KnowledgeGraph()

    time_start = datetime.now()  # è®°å½•å¼€å§‹æ—¶é—´
    
    summary =  ai_chat(prompt_summary(text=text2),model="gpt-4o-mini")
    print(summary)
    prompt = prompt_extract(text=summary)
    result =  ai_chat(prompt,model="gpt-4o-mini")
    # result = ai_chat(prompt,model="gpt-4o")
    print(result)

    all_records: dict[int, str] = {}
    all_records[1] = result

    summary =  ai_chat(prompt_summary(text=text),model="gpt-4o-mini")
    print(summary)
    prompt = prompt_extract(text=summary)
    result =  ai_chat(prompt,model="gpt-4o-mini")
    # result = ai_chat(prompt,model="gpt-4o")
    print(result)

    all_records[2] = result
    results = await graph_extractor.process_results(all_records)
    
    print(results)
    # visualize_and_save_graph(results, './graphtest.png')


    time_end = datetime.now()  # è®°å½•ç»“æŸæ—¶é—´
    print(f"{round((time_end - time_start).total_seconds(), 2)}s")


if __name__ == "__main__":
    asyncio.run(main())